---
title: "Beyond consistency: Contextual dependency of language style in monologue and
  conversation"
output:
  html_document:
    keep_md: yes
    number_sections: yes

---

This R markdown provides the data preparation for our forthcoming manuscript.

To run this from scratch, you will need the following files:

* [This is where a description of the data setup goes]
* `./scripts/bc-libraries_and_functions.r`: Loads in necessary libraries and
creates new functions for our analyses.

**Code written by**: L. C. Mueller-Frommeyer (Technische Universitaet
Braunschweig) & A. Paxton (University of Connecticut)

**Date last modified**: 14 June 2019

```{r silent-setup, include=FALSE}

# cache our results
library(knitr)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.lazy=FALSE)

```

***

# Preliminaries

```{r prelim, warning = FALSE, error = FALSE, message = FALSE}

# clear everything
rm(list=ls())

# load libraries and add new functions
source('./scripts/bc-libraries_and_functions.r')

```

***

# Hypothesis 1

***

## Data preparation

*** 

### Recurrence quantification analysis: Monologues

```{r monologue-load-data}

# read in all monologue files
mon_files = list.files('./data/LIWC-results/RQA/Monologues',
                       pattern = ".txt", full.names = TRUE)
mon_dfs = plyr::ldply(mon_files,
                      read.table, sep="\t", dec = ",", header=TRUE) #added decimal to get numbers instead of characters

```

```{r monologue-prepare-for-rqa}

# prepare monologues for RQA
mon_dfs = mon_dfs %>%

  # separate 'Filename' column into separate columns
  tidyr::separate(Filename,
                  into = c("dyad_id", "dyad_position", "speaker_code"),
                  sep = '_',
                  remove = FALSE,
                  extra = "drop",
                  fill = "warn") %>%

  # extract speaker number ID and conversation type from variable
  mutate(cond = gsub("[[:digit:]]+","",dyad_id)) %>%

  # create new variable function_contrast with all 0 replaced by -1
  dplyr::rename(function_words = function.) %>%
  mutate(function_contrast = dplyr::if_else(function_words==0,
                                            -1,
                                            function_words)) %>%

  #add new variable specifying conversation type
  mutate(conv.type = "M")

```

```{r monologues-rqa}

# split dataframe by monologue
split_mon = split(mon_dfs, list(mon_dfs$Filename))

# cycle through the individual monologues
crqa_results_mon = data.frame()
for (next_mon in split_mon){

  # run (auto-)recurrence
  rqa_for_mon = crqa(ts1=next_mon$function_words,
                     ts2=next_mon$function_contrast,
                     delay=1,
                     embed=1,
                     r=0.1,
                     normalize=0,
                     rescale=0,
                     mindiagline=2,
                     minvertline=2,
                     tw=1, # exclude line of identity
                     whiteline=FALSE,
                     recpt=FALSE)

  # save plot-level information to dataframe
  dyad_id = unique(next_mon$dyad_id)
  speaker_code = unique(next_mon$speaker_code)
  cond = NA   #changed it to NA as there was no condition in the monologue
  conv.type = unique(next_mon$conv.type)
  next_data_line = data.frame(dyad_id,  
                              speaker_code,
                              conv.type,
                              cond,
                              rqa_for_mon[1:9])
  crqa_results_mon = rbind.data.frame(crqa_results_mon,next_data_line)
}

```

***

### Recurrrence quantification analysis: Conversations

```{r conversations-load-data}

# read in all conversation files
conv_files = list.files('./data/LIWC-results/RQA/Conversations-1',
                        pattern = ".txt", full.names = TRUE)
conv_dfs = plyr::ldply(conv_files,
                       read.table, sep="\t", dec = ",", header=TRUE)

```

```{r conversations-prepare-for-crqa}

# prepare conversations for RQA
conv_dfs = conv_dfs %>%

  # separate 'Filename' column into separate columns
  tidyr::separate(Filename,
                  into = c("dyad_id", "dyad_position", "speaker_code"),
                  sep = '_',
                  remove = FALSE,
                  extra = "drop",
                  fill = "warn") %>%

  # extract speaker number ID and conversation type from variable
  mutate(cond = gsub("[[:digit:]]+","",dyad_id)) %>%

  # create new variable function_contrast with all 0 replaced by -1
  dplyr::rename(function_words = function.) %>%
  mutate(function_contrast = dplyr::if_else(function_words==0,
                                            -1,
                                            function_words)) %>%

  # add new variable specifying conversation type
  mutate(conv.type = "C")

```

```{r conversations-crqa}

# split dataframe by conversation
split_conv = split(conv_dfs, list(conv_dfs$Filename))

# cycle through the individual conversations
crqa_results_conv = data.frame()
for (next_conv in split_conv){

  # run cross-recurrence
  rqa_for_conv = crqa(ts1=next_conv$function_words,
                      ts2=next_conv$function_contrast,
                      delay=1,
                      embed=1,
                      r=0.1,
                      normalize=0,
                      rescale=0,
                      mindiagline=2,
                      minvertline=2,
                      tw=0,
                      whiteline=FALSE,
                      recpt=FALSE)

  # save plot-level information to dataframe
  dyad_id = unique(next_conv$dyad_id)
  speaker_code = unique(next_conv$speaker_code)
  conv.type = unique(next_conv$conv.type)
  cond = unique(next_conv$cond)
  next_data_line = data.frame(dyad_id,  
                              speaker_code,
                              conv.type,
                              cond,
                              rqa_for_conv[1:9])
  crqa_results_conv = rbind.data.frame(crqa_results_conv,next_data_line)
}

```

***

### Create dataframe for H1

```{r create-h1-dataframe}

# bring together the monologue and conversation data
h1_data = rbind(crqa_results_mon, crqa_results_conv)

#save results to file
write.table(h1_data,'./data/h1_data.csv',sep=",")

```

***

## Data analysis

Here, we perform a linear mixed-effects model to analyze how conversation
type---whether a monologue (M) or conversation (C)---changes a person's 
language style, specifically looking at their use of function words (often a
measure of syntactic complexity and structure).

We attempted to analyze the data using maximal random effects structures and
an uncorrelated random intercept within the random slope, but both models 
failed to converge. As a result, we use only the random intercept in our model.

```{r model-h1}

# does linguistic style change based on the conversational context?
h1_analyses <- lmer(RR ~ conv.type + (1|speaker_code), data = h1_data, REML = FALSE)

```

```{r silently-print-summary, eval=TRUE, echo=TRUE, include=FALSE}

# print summary table
summary(h1_analyses)

```

```{r silently-print-table, eval=TRUE, echo=TRUE, include=FALSE}

# neatly print output
pander_lme(h1_analyses)

```

***

### Post-hoc analysis: Changes by conversation type

Next, we will do a post-hoc analysis to see how language style differs from the
monologues based on each condition.

```{r}

```


***

# Hypothesis 2

***

## Data preparation

***

### Data cleaning: Speaker A

```{r speaker-a-files}

# get list of Conversation files for Speaker A
A_files = list.files('./data/LIWC-results/cRQA/SpeakerA',
                     pattern = ".txt", full.names = TRUE)
A_dfs = plyr::ldply(A_files,
                    read.table, sep="\t", dec = ",", header=TRUE) #added decimal to get numbers instead of characters

```

```{r speaker-a-prep-for-crqa}

# prepare conversations Speaker A for CRQA
A_dfs = A_dfs %>% ungroup() %>%

  # separate 'Filename' column into separate columns
  tidyr::separate(Filename,
                  into = c("dyad_id", "dyad_position",  "speaker_code"),
                  sep = '_',
                  remove = FALSE,
                  extra = "drop",
                  fill = "warn") %>%

  # extract speaker number ID and conversation type from variable
  mutate(cond = gsub("[[:digit:]]+","",dyad_id)) %>%
  mutate(dyad_id = gsub("[K|P]","",dyad_id)) %>%

  # rename function. to function_words
  dplyr::rename(function_words = function.) %>%

  # group by participant to cut quantiles
  group_by(Filename) %>%

  # recode quartiles
  mutate(fw_quantiles = as.numeric(
    gtools::quantcut(function_words,
                     q=4,
                     na.rm = TRUE))
  ) %>% ungroup()  %>%
  
  # recode anytime there is 0 function word use
  mutate(fw_quantiles = dplyr::if_else(function_words==0,
                                       0,
                                       fw_quantiles)) %>%
  
  # specify these data are real
  mutate(data_type = 'real') %>%

  # rename to specify speaker A contributions
  dplyr::rename(function_words_A = function_words,
                fw_quantiles_A = fw_quantiles,
                speaker_A = speaker_code,
                wc_A = WC) %>%
  dplyr::select(-Filename, -dyad_position)

```

***

### Data cleaning: Speaker B

```{r speaker-b-files}

# get list of Conversation files for Speaker B
B_files = list.files('./data/LIWC-results/cRQA/SpeakerB',
                     pattern = ".txt", full.names = TRUE)
B_dfs = plyr::ldply(B_files,
                    read.table, sep="\t", dec = ",", header=TRUE) #added decimal to get numbers instead of characters

```

```{r speaker-b-prep-for-crqa}

# prepare conversations Speaker B for CRQA
B_dfs = B_dfs %>% ungroup() %>%

  # separate 'Filename' column into separate columns
  tidyr::separate(Filename,
                  into = c("dyad_id", "dyad_position",  "speaker_code"),
                  sep = '_',
                  remove = FALSE,
                  extra = "drop",
                  fill = "warn") %>%

  # extract speaker number ID and conversation type from variable
  mutate(cond = gsub("[[:digit:]]+","",dyad_id)) %>%
  mutate(dyad_id = gsub("[K|P]","",dyad_id)) %>%

  # rename function. to function_words
  dplyr::rename(function_words = function.) %>%

  # group by participant to cut quantiles
  group_by(Filename) %>%

  # recode quartiles
  mutate(fw_quantiles = as.numeric(
    gtools::quantcut(function_words,
                     q=4,
                     na.rm = TRUE))
  ) %>% ungroup() %>%
  
  # specify these data are real
  mutate(data_type = 'real')

```

***

## Create surrogate time series through shuffling time series

```{r speaker-a-create-surrogate}

# split Speaker As by dyad_id
split_As = split(A_dfs, list(A_dfs$dyad_id))
surrogate_crqa = data.frame()
for (next_conv in split_As){
  
  # preserve the original dataframe
  next_conv_real = next_conv
  
  # permute 10 times for baseline
  permuted_df = data.frame()
  for (run in c(1:10)){
    next_shuffle = next_conv_real %>%
      
      # shuffle each person's linguistic contributions
      group_by(Filename) %>%
      mutate(sur_fw_quantiles = gtools::permute(fw_quantiles)) %>%
      ungroup() %>%
      
      # add a marker for what run we're on
      mutate(run = run) %>%
      mutate(data_type = surrogate)
    
  # specify these data are real
  mutate(data_type = 'real') %>%
  
  # rename to specify speaker A contributions
  dplyr::rename(function_words_B = function_words,
                fw_quantiles_B = fw_quantiles,
                speaker_B = speaker_code,
                wc_B = WC) %>%
  dplyr::select(-Filename, -dyad_position)

#Create surrogate data for speaker B
```

***

### Cross-recurrence quantification analysis

Before we run CRQA, we must first combine the Speaker A and Speaker B
dataframes. This requires truncating the data so that both speakers have the
same number of turns in each conversation and converting from longform data
(i.e., creating columns for speaker identifier [A vs. B], function words, and
turn) to wideform data (i.e., creating columns for Speaker A function words,
Speaker B function words, and turn).

```{r merge-and-prepare-speaker-dataframes}

# merge Speaker A and Speaker B together
df_final = rbind.data.frame(A_dfs, B_dfs) %>%

  # use the newest tidyr branch to widen by multiple variables
  select(-Filename)%>%
  tidyr::pivot_wider(names_from=dyad_position,
                     values_from=c("function_words",
                                   "fw_quantiles",
                                   "speaker_code",
                                   "WC")) %>%
  
  # because any missing values are filled with `NA`, 
  # we can truncate turns simply by dropping `NA`
  tidyr::drop_na()

#repeat for surrogate body

```

```{r}

# split dataframe by conversation
split_conv = split(df_final, list(df_final$dyad_id))

# cycle through the individual conversations
crqa_results = data.frame()
for (next_conv in split_conv){

  # run cross-recurrence
  rqa_for_conv = crqa(ts1=next_conv$fw_quantiles_A,
                      ts2=next_conv$fw_quantiles_B,
                      delay=1,
                      embed=1,
                      r=0.1,
                      normalize=0,
                      rescale=0,
                      mindiagline=2,
                      minvertline=2,
                      tw=0,
                      whiteline=FALSE,
                      recpt=FALSE)

  # save plot-level information to dataframe
  dyad_id = unique(next_conv$dyad_id)
  cond = unique(next_conv$cond)
  next_data_line = data.frame(dyad_id,  
                              cond,
                              rqa_for_conv[1:9])
  crqa_results = rbind.data.frame(crqa_results,next_data_line)
}

#repeat for surrogate body

```

```{r}

# cycle through all dyads
drp_results = data.frame()
for (next_conv in split_conv) {
  
   # calculate diagonal recurrence profile using categorical recurrence
  drp_for_conv = drpdfromts(next_conv$fw_quantiles_A, next_conv$fw_quantiles_B, ws = wsz, datatype="categorical")


 # save plot-level information to dataframe
  
    dyad_id = unique(next_conv$dyad_id)
    cond = unique(next_conv$cond)
    drps = data.frame(dyad_id  = rep(dyad_id(length(raw))), cond = rep(cond(length(raw))), raw = timeVals$raw, ot1 = timeVals$ot1, ot2  = timeVals$ot2, rr = drp_for_conv$profile)
    drp_results = rbind.data.frame(drp_results, drps)
}

#descriptives are still missing - didn't know what to do with this
#output should still have several observations per dyad -- one per lag, is that correct?

#repeat for surrogate body

```

***

## Data analysis

```{r}

# rename variables and center the binary variables
drp_results = drp_results %>% ungroup() %>%
  plyr::rename(.,
               c("cond"="condition")) %>%
    # first-order polynomials
  mutate(condition.ot1 = condition * ot1) %>%

  # second-order polynomials
  mutate(condition.ot2 = condition * ot2) %>%

  # polynomial interactions
  mutate(ot1.ot2 = ot1 * ot2) %>%
  mutate(condition.ot1.ot2 = condition * ot1 * ot2) 

#repeat for surrogate body
  
```  

***

## Create standardized dataframe

Let's create a new dataframe with all standardized variables. This allows us to interpret the resulting values as effect sizes (see Keith, 2005, *Multiple regression and beyond*).

```{r standardized-dataframe, eval = FALSE}

# standardize all variables
drp_st = mutate_each(drp_results,funs(as.numeric(scale(.))))

#repeat for surrogate body
```

***

#Analyses H2

We now create a linear mixed-effects model to gauge how linear lag (`ot1`) and quadratic lag (`ot2`) interact with condition (`condition`) to influence Language Style Matching (`rr`). We present both standardized and raw models below.

```{r central-model, warning=FALSE, error=FALSE, message=FALSE}

# standardized maximal random-effects model
H2_st = lmer(rr ~ condition + ot1 + ot2 + ot1.ot2 + condition.ot1 + condition.ot2 + condition.ot1.ot2 +  
               (1 condition.ot1.ot2 | dyad_id),
                                        #what is the maximum random effects structure for this?
                                      
                                    data=drp_st, REML=FALSE)



# raw maximal random-effects model
H2_raw = lmer(rr ~ condition + ot1 + ot2 + ot1.ot2 + condition.ot1 + condition.ot2 + condition.ot1.ot2 + 
                                        #random effects are still missing here
                                      
                                    data=drp_results, REML=FALSE)

#repeat for surrogate body
```

# List of next steps
* create baseline surrogate measures for H2
* calculate diagonal recurrence plots for H2
* start code for analyses for H1 and H2 


* previous examples
* look over example of using categorical CRQA from emotion dynamics paper:
https://github.com/a-paxton/emotion-dynamics/blob/master/get_rqa_measures.R
* another useful example will be continuous CRQA in dual conversation constraints paper: https://github.com/a-paxton/dual-conversation-constraints/blob/a167f004c71d9637ec30082de13a1ba6283846bb/dual-conversation-constraints.Rmd#L309 (direct link to line)
#for RQA and CRQA, update so that we have all of the variables we need in the
eventual dataframes (compare with the `mon_dfs` and figure out which to
preserve)
#fix CRQA based on fixes to RQA today
#add variable to monologue and dialogue dataframes to specify monologue v.
#dialogue
# add a step to save the eventual table
# add a step to save the eventual table
# start doing categorical CRQA
# create quartiles of function words -- function `quantile` - 'quantcut'
# recode 0 appearances of function words as something else (e.g., 0 for one participant, -1 for the other)
# run categorical CRQA with `tw=0` over all of these


* create baseline surrogate measures for H2
