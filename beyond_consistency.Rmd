---
title: "Beyond consistency: Contextual dependency of language style in monologue and
  conversation"
output:
  html_document:
    keep_md: yes
    number_sections: yes

---

This R markdown provides the data preparation and analyses for our 
manuscript, "Beyond consistency: Contextual dependency of language 
style in monologue and conversation" (Mueller-Frommeyer, Kauffeld, & 
Paxton, *accepted*, *Cognitive Science*).

To run this from scratch, you will need the following files:

* The data is accessible in our OSF project (https://osf.io/3jx2g/). 
  Please create a folder called `data`, with a subfolder `LIWC-results`, 
  with a subfolder `RQA` with two sub-folders `monologues` and `conversations` 
  in your working directory. Add the data from OSF to the monologue and 
  conversation folders.
* `./scripts/bc-libraries_and_functions.r`: Loads in necessary libraries and
  creates new functions for our analyses.

**Code written by**: L. C. Mueller-Frommeyer (Technische Universitaet
Braunschweig) & A. Paxton (University of Connecticut)

**Date last modified**: 06 April 2020

```{r silent-setup, include=FALSE}

# cache our results
library(knitr)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.lazy=FALSE)

```

***

# Preliminaries

```{r prelim, warning = FALSE, error = FALSE, message = FALSE}

# clear everything
rm(list=ls())

# load libraries and add new functions
source('./scripts/bc-libraries_and_functions.r')

```

***

# Data preparation

***

## Recurrence quantification analysis

### Monologues

```{r monologue-load-data}

# read in all monologue files
mon_files = list.files('./data/LIWC-results/RQA/Monologues',
                       pattern = ".txt", full.names = TRUE)
mon_dfs = plyr::ldply(mon_files,
                      read.table, sep="\t", dec = ",", header=TRUE) #added decimal to get numbers instead of characters

```

```{r monologue-prepare-for-rqa}

# prepare monologues for RQA
mon_dfs = mon_dfs %>%
  
  # separate 'Filename' column into separate columns
  tidyr::separate(Filename,
                  into = c("dyad_id", "dyad_position", "speaker_code"),
                  sep = '_',
                  remove = FALSE,
                  extra = "drop",
                  fill = "warn") %>%
  
  
  # extract speaker number ID and conversation type from variable
  mutate(cond = gsub("[[:digit:]]+","",dyad_id)) %>%
  
  # create new variable function_contrast with all 0 replaced by -1
  dplyr::rename(function_words = function.) %>%
  mutate(function_contrast = dplyr::if_else(function_words==0,
                                            -1,
                                            function_words)) %>%
  
  #add new variable specifying conversation type
  mutate(conv.type = "M")

```

```{r monologues-rqa}

# split dataframe by monologue
split_mon = split(mon_dfs, list(mon_dfs$Filename))

# cycle through the individual monologues
rqa_mon = data.frame()
for (next_mon in split_mon){
  
  # run (auto-)recurrence
  rqa_for_mon = crqa(ts1=next_mon$function_words,
                     ts2=next_mon$function_contrast,
                     delay=1,
                     embed=1,
                     r=0.1,
                     normalize=0,
                     rescale=0,
                     mindiagline=2,
                     minvertline=2,
                     tw=1, # exclude line of identity
                     whiteline=FALSE,
                     recpt=FALSE)
  
  # save plot-level information to dataframe
  dyad_id = unique(next_mon$dyad_id)
  speaker_code = unique(next_mon$speaker_code)
  cond = NA   #changed it to NA as there was no condition in the monologue
  conv.type = unique(next_mon$conv.type)
  next_data_line = data.frame(dyad_id,  
                              speaker_code,
                              conv.type,
                              cond,
                              rqa_for_mon[1:9]) %>%
    mutate(rNRLINE = NRLINE / dim(next_mon)[1]) # normalize NRLINE by number of words
  rqa_mon = rbind.data.frame(rqa_mon,next_data_line)
  
  # save the RPs -- including LOI/LOS for plotting
  # rqa_for_mon = crqa(ts1=next_mon$function_words,
  #                    ts2=next_mon$function_contrast,
  #                    delay=1,
  #                    embed=1,
  #                    r=0.1,
  #                    normalize=0,
  #                    rescale=0,
  #                    mindiagline=2,
  #                    minvertline=2,
  #                    tw=0, # include LOI/LOS
  #                    whiteline=FALSE,
  #                    recpt=FALSE)
  # png(filename = paste0('./figures/monologue/rp-speaker_',speaker_code,'-monologue.png'))
  # plotRP(rqa_for_mon$RP,
  #        list(unit = 2, labelx = "Speaker A", labely = "Speaker A",
  #             cols = "black", pcex = .5))
  # dev.off()
}

# clean up what we don't need
rm(split_mon, next_mon, rqa_for_mon,
   dyad_id, speaker_code, cond, conv.type, next_data_line)

```

### Conversations

```{r conversations-load-data}

# read in all conversation files
conv_files = list.files('./data/LIWC-results/RQA/Conversations-1',
                        pattern = ".txt", full.names = TRUE)
conv_dfs = plyr::ldply(conv_files,
                       read.table, sep="\t", dec = ",", header=TRUE)

```

```{r conversations-prepare-for-rqa}

# prepare conversations for RQA
conv_dfs = conv_dfs %>%
  
  # separate 'Filename' column into separate columns
  tidyr::separate(Filename,
                  into = c("dyad_id", "dyad_position", "speaker_code"),
                  sep = '_',
                  remove = FALSE,
                  extra = "drop",
                  fill = "warn") %>%
  
  # extract speaker number ID and conversation type from variable
  mutate(cond = gsub("[[:digit:]]+","",dyad_id)) %>%
  
  # create new variable function_contrast with all 0 replaced by -1
  dplyr::rename(function_words = function.) %>%
  mutate(function_contrast = dplyr::if_else(function_words==0,
                                            -1,
                                            function_words)) %>%
  
  # add new variable specifying conversation type
  mutate(conv.type = "C")

```

```{r conversations-rqa}

# split dataframe by conversation
split_conv = split(conv_dfs, list(conv_dfs$Filename))

# cycle through the individual conversations
rqa_conv = data.frame()
for (next_conv in split_conv){
  
  # run recurrence
  rqa_for_conv = crqa(ts1=next_conv$function_words,
                      ts2=next_conv$function_contrast,
                      delay=1,
                      embed=1,
                      r=0.1,
                      normalize=0,
                      rescale=0,
                      mindiagline=2,
                      minvertline=2,
                      tw=1, # exclude line of identity
                      whiteline=FALSE,
                      recpt=FALSE)
  
  # save plot-level information to dataframe
  dyad_id = unique(next_conv$dyad_id)
  speaker_code = unique(next_conv$speaker_code)
  conv.type = unique(next_conv$conv.type)
  cond = unique(next_conv$cond)
  next_data_line = data.frame(dyad_id,  
                              speaker_code,
                              conv.type,
                              cond,
                              rqa_for_conv[1:9]) %>%
    mutate(rNRLINE = NRLINE / dim(next_conv)[1]) # normalize NRLINE by number of words
  rqa_conv = rbind.data.frame(rqa_conv,next_data_line)
  
  # # plot the RPs -- include LOI/LOS
  # rqa_for_conv = crqa(ts1=next_conv$function_words,
  #                     ts2=next_conv$function_contrast,
  #                     delay=1,
  #                     embed=1,
  #                     r=0.1,
  #                     normalize=0,
  #                     rescale=0,
  #                     mindiagline=2,
  #                     minvertline=2,
  #                     tw=0, # retain LOI for plotting only
  #                     whiteline=FALSE,
  #                     recpt=FALSE)
  # png(filename = paste0('./figures/conversation/rp-speaker_',speaker_code,'-conversation.png'))
  # plotRP(rqa_for_conv$RP,
  #                  list(unit = 2, labelx = "Speaker A", labely = "Speaker A",
  #                       cols = "black", pcex = .01))
  # dev.off()
}

# clean up what we don't need
rm(split_conv, next_conv, rqa_for_conv,
   dyad_id, speaker_code, conv.type, cond, next_data_line)

```

***

## Proportions

### Monologues

```{r extract-function-word-frequency-per-speaker-for-monologues}

# split monologues into separate dataframes
split_mon = split(mon_dfs, list(mon_dfs$Filename))

# create an empty frame for results
FW_freq_mon = data.frame()

# cycle through all monologues
for (next_mon in split_mon) {
  
  # count frequency of function words
  freq = sum(next_mon$function_words == 100)
  wc = sum(next_mon$WC == 1)
  speaker_code = unique(next_mon$speaker_code)
  
  # add results to dataframe
  data_temp = data.frame(freq, wc, speaker_code)
  
  
  FW_freq_mon = rbind.data.frame(FW_freq_mon, data_temp)
}
FW_freq_mon$prop = FW_freq_mon$freq/FW_freq_mon$wc

```

### Conversations

```{r extract-function-word-frequency-per-speaker-for-conversations}

# split conversations into separate dataframes
split_conv = split(conv_dfs, list(conv_dfs$Filename))

# create an empty frame for results
FW_freq_conv = data.frame()

# cycle through all conversations
for (next_conv in split_conv) {
  
  # count frequency of function words
  freq = sum(next_conv$function_words == 100)
  wc = sum(next_conv$WC == 1)
  speaker_code = unique(next_conv$speaker_code)
  
  # add results to dataframe
  data_temp = data.frame(freq, wc, speaker_code)
  FW_freq_conv = rbind.data.frame(FW_freq_conv, data_temp)
}

# convert from frequency to proportion
FW_freq_conv$prop = FW_freq_conv$freq/FW_freq_conv$wc

```

***

## Create dataframes

We'll need to create two dataframes: an unstandardized dataframe
(`analysis_df`) and a standardized one (`standardized_df`).

```{r create-analysis-dataframe}


# bring together monologue rqa and freq data
data_mon = dplyr::left_join(rqa_mon, FW_freq_mon, by = "speaker_code")

# bring togehter conversation rqa and freq data
data_conv = dplyr::left_join(rqa_conv, FW_freq_conv, by = "speaker_code")

# bring together the monologue and conversation data
analysis_df = rbind(data_mon, data_conv) %>%
  
  # update coding for conversation type and condition
  mutate(conv.type = as.factor(dplyr::if_else(conv.type == "M",
                                              -.5,
                                              .5)),
         cond = as.factor(dplyr::if_else(cond == "P",
                                         -.5,
                                         .5)))

# save dataframe to file
write.table(analysis_df, './data/analysis_df.csv',
            sep=",", row.names=FALSE, col.names=TRUE)

```

```{r create-standardized-dataframe}

# standardize the analysis dataframe
standardized_df = analysis_df %>%
  
  # convert things as needed to numeric
  mutate(dyad_id = as.numeric(as.factor(dyad_id)),
         speaker_code = as.numeric(as.factor(speaker_code))) %>%
  
  # standardize
  mutate_all(funs(as.numeric(scale(as.numeric(.))))) %>%
  
  # convert to factors as needed
  mutate(dyad_id = as.factor(dyad_id),
         speaker_code = as.factor(speaker_code),
         conv.type = as.factor(conv.type),
         cond = as.factor(cond))

# save dataframe to file
write.table(standardized_df, './data/standardized_df.csv',
            sep=",", row.names=FALSE, col.names=TRUE)

```

***

# Data analysis

***
## Power Sensitivity Analyses

## Power analyses for d = .4

First, we run power sensitivity analysis for our model using recommendations from
Brysbaert and Stevens (2018). We run the power analysis for the standardized
model using effect sizes between d = .4 and d = .1 to detect the smallest possible 
effect size that can be found with a power of at least 80%.

```{r, eval=TRUE, results="hide", message = FALSE}

# power analyses for the standardized model
planned_analyses_st <- lmer(RR ~ conv.type + (1|speaker_code),
                            data = standardized_df, REML = FALSE)

# change the fixed effect to the assumed effect size of .4 (Brysbaert & Stevens, 2018)
#   (although with a negative sign)
fixef(planned_analyses_st)["conv.type0.997879106838302"] = -0.4

# run power analysis for our current sample, with a default significance
#   criterion of .05 and an effect size of .4
power.4 <- powerSim(planned_analyses_st,
                  nsim = 1000)

```

```{r, echo=FALSE}

power.4

```


## Power analyses for d = .3

First, we run a power analysis for our model using recommendations from
Brysbaert and Stevens (2018). We run the power analysis for the standardized
model.

```{r, eval=TRUE, results="hide", message = FALSE}

# power analyses for the standardized model
planned_analyses_st <- lmer(RR ~ conv.type + (1|speaker_code),
                            data = standardized_df, REML = FALSE)

# change the fixed effect to the assumed effect size of .4 (Brysbaert & Stevens, 2018)
#   (although with a negative sign)
fixef(planned_analyses_st)["conv.type0.997879106838302"] = -0.3

# run power analysis for our current sample, with a default significance
#   criterion of .05 and an effect size of .4
power.3 <- powerSim(planned_analyses_st,
                  nsim = 1000)

```

```{r, echo=FALSE}

power.3

```

## Additional Power analyses d = .29 to see if the power is below 80% 

First, we run a power analysis for our model using recommendations from
Brysbaert and Stevens (2018). We run the power analysis for the standardized
model.

```{r, eval=TRUE, results="hide", message = FALSE}

# power analyses for the standardized model
planned_analyses_st <- lmer(RR ~ conv.type + (1|speaker_code),
                            data = standardized_df, REML = FALSE)

# change the fixed effect to the assumed effect size of .4 (Brysbaert & Stevens, 2018)
#   (although with a negative sign)
fixef(planned_analyses_st)["conv.type0.997879106838302"] = -0.29

# run power analysis for our current sample, with a default significance
#   criterion of .05 and an effect size of .4
power.29 <- powerSim(planned_analyses_st,
                  nsim = 1000)

```

```{r, echo=FALSE}

power.29

```


## Power analyses for d = .2

First, we run a power analysis for our model using recommendations from
Brysbaert and Stevens (2018). We run the power analysis for the standardized
model.

```{r, eval=TRUE, results="hide", message = FALSE}

# power analyses for the standardized model
planned_analyses_st <- lmer(RR ~ conv.type + (1|speaker_code),
                            data = standardized_df, REML = FALSE)

# change the fixed effect to the assumed effect size of .4 (Brysbaert & Stevens, 2018)
#   (although with a negative sign)
fixef(planned_analyses_st)["conv.type0.997879106838302"] = -0.2

# run power analysis for our current sample, with a default significance
#   criterion of .05 and an effect size of .4
power.2 <- powerSim(planned_analyses_st,
                  nsim = 1000)

```

```{r, echo=FALSE}

power.2

```

## Power analyses for d = .1

First, we run a power analysis for our model using recommendations from
Brysbaert and Stevens (2018). We run the power analysis for the standardized
model.

```{r, eval=TRUE, results="hide", message = FALSE}

# power analyses for the standardized model
planned_analyses_st <- lmer(RR ~ conv.type + (1|speaker_code),
                            data = standardized_df, REML = FALSE)

# change the fixed effect to the assumed effect size of .4 (Brysbaert & Stevens, 2018)
#   (although with a negative sign)
fixef(planned_analyses_st)["conv.type0.997879106838302"] = -0.1

# run power analysis for our current sample, with a default significance
#   criterion of .05 and an effect size of .4
power.1 <- powerSim(planned_analyses_st,
                  nsim = 1000)

```

```{r, echo=FALSE}

power.1

```

***

## Descriptives and correlation tables

Here we generate descriptive statistics and intercorrelations for our
variables, separated by monologue and conversation. Data are saved to
files and reported as Supplementary Materials for the paper.

```{r calculation of descriptives}

# Prepare data for descriptive analysis - only leave numerical elements in data frame
Descriptives_mon =  data_mon %>%
  select(-dyad_id, -speaker_code, -conv.type, -cond, -freq, -wc, -NRLINE, -ENTR)
Descriptives_conv =  data_conv %>%
  select(-dyad_id, -speaker_code, -conv.type, -cond, -freq, -wc, -NRLINE, -ENTR)

# Mean, Min, Max for monologues and dialogues
mean_mon = psych::describe(Descriptives_mon)
mean_conv = psych::describe(Descriptives_conv)

# write descriptives to files
write.table(mean_mon, './data/mean_mon.csv',
            sep=",", row.names=FALSE, col.names=TRUE)
write.table(mean_conv, './data/mean_conv.csv',
            sep=",", row.names=FALSE, col.names=TRUE)

# Intercorrelations for monologues
p_cor_mon = rcorr(as.matrix(Descriptives_mon))
p_mon = p_cor_mon$P
r_mon = p_cor_mon$r

# Intercorrelations for dialogues
p_cor_conv = rcorr(as.matrix(Descriptives_conv))
p_conv = p_cor_conv$P
r_conv = p_cor_conv$r

# save intercorelations to file
write.table(p_mon, './data/p_mon.csv',
            sep=",", row.names=FALSE, col.names=TRUE)
write.table(r_mon, './data/r_mon.csv',
            sep=",", row.names=FALSE, col.names=TRUE)
write.table(p_conv, './data/p_conv.csv',
            sep=",", row.names=FALSE, col.names=TRUE)
write.table(r_conv, './data/r_conv.csv',
            sep=",", row.names=FALSE, col.names=TRUE)


```

***

## Planned analysis

Here, we perform a linear mixed-effects model to analyze how conversation
setting---whether a monologue (M) or conversation (C)---changes a person's
language style, specifically looking at their use of function words (often a
measure of syntactic complexity and structure).

We attempted to analyze the data using maximal random effects structures and
an uncorrelated random intercept within the random slope, but both models
failed to converge. As a result, we use only the random intercept in our
models. We include both raw and standardized models below.

***

### RR: Raw model

```{r main-model-raw, results="hide", message = FALSE}

# raw: does linguistic style change based on the conversation setting?
planned_analyses_raw <- lmer(RR ~ conv.type + (1|speaker_code),
                             data = analysis_df, REML = FALSE)

# calculate 95% CI for model
CI_planned_analyses_raw <- confint(planned_analyses_raw)


# calculate effect size for model
es_planned_analyses_raw <- lme.dscore(planned_analyses_raw,
                                      data = analysis_df, type = "lme4")
```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(planned_analyses_raw)

# print effect sizes
es_planned_analyses_raw

# print confidence intervals
CI_planned_analyses_raw

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(planned_analyses_raw)

```

***

### RR: Standardized model

```{r, results="hide", message = FALSE}

# standardized: does linguistic style change based on the conversation setting?
planned_analyses_st <- lmer(RR ~ conv.type + (1|speaker_code),
                            data = standardized_df, REML = FALSE)

# calculate 95%CI for model
CI_planned_analyses_st <- confint(planned_analyses_st)

# calculate effect size for model
es_planned_analyses_st <- lme.dscore(planned_analyses_st,
                                     data = standardized_df, type = "lme4")

```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(planned_analyses_st)

# print effect sizes
es_planned_analyses_st

# print confidence intervals
CI_planned_analyses_st

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(planned_analyses_st)

```

As predicted, we do see a difference in linguistic style between conversations
and monologues. Specifically, we find that conversations tend to have *more*
recurrence in their use of function words than monologues do.

***

## Validating RR against proportion

To validate the use of RQA (dynamic) metrics against standard (static)
metrics, we next build a similar model predicting proportion with
conversation type. If RR and proportion tap into similar underlying
dynamics, we would expect to see congruent results between the two
methods.

Also, we calculated a model for a more traditional static measure of language style-
namely, proportion of function words to compare the new dynamic approach to
the traditional more static approach.

***

### Raw model

```{r exploratory-prop-model-raw, results="hide", message = FALSE}

# raw: does linguistic style change based on the conversation setting?
exploratory_analyses_raw_prop <- lmer(prop ~ conv.type + (1|speaker_code),
                                      data = analysis_df, REML = FALSE)

# calculate 95% CI for model
CI_exploratory_analyses_raw_prop <- confint(exploratory_analyses_raw_prop)


# calculate effect size for model
es_exploratory_analyses_raw_prop <- lme.dscore(exploratory_analyses_raw_prop,
                                               data = analysis_df, type = "lme4")
```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_raw_prop)

# print effect sizes
es_exploratory_analyses_raw_prop

# print confidence intervals
CI_exploratory_analyses_raw_prop

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_raw_prop)

```

### Standardized model

```{r, results="hide", message = FALSE}

# standardized: does linguistic style change based on the conversation setting?
exploratory_analyses_st_prop <- lmer(prop ~ conv.type + (1|speaker_code),
                                     data = standardized_df, REML = FALSE)

# calculate 95%CI for model
CI_exploratory_analyses_st_prop <- confint(exploratory_analyses_st_prop)

# calculate effect size for model
es_exploratory_analyses_st_prop <- lme.dscore(exploratory_analyses_st_prop,
                                              data = standardized_df, type = "lme4")

```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_st_prop)

# print effect sizes
es_exploratory_analyses_st_prop

# print confidence intervals
CI_exploratory_analyses_st_prop

```


```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_st_prop)

```

We see a difference in proportion of function words between conversations
and monologues. Specifically, we find that conversations tend to have a *higher*
proportion of function words than monologues do.

The congruence between proportion-based models and RR-based models support
the validity of the latter. However, while proportion can only give a single
value of similarity, we can tap into other RQA metrics to understand the
structure of those patterns.

***

## Additional exploratory analyses

Next, we repeat our analyses for RQA parameters indicative of LS structure 
and complexity to explore the influence of conversational setting on aspects of
language style structure and complexity in addition to recurrence. An explanation of
the parameters under investigation can be found in the manuscript.

***

### DET: Exploratory analyses

#### Raw model

```{r exploratory-det-model-raw, results="hide", message = FALSE}

# raw: does linguistic style change based on the conversation setting?
exploratory_analyses_raw_DET <- lmer(DET ~ conv.type + (1|speaker_code),
                                     data = analysis_df, REML = FALSE)

# calculate 95% CI for model
CI_exploratory_analyses_raw_DET <- confint(exploratory_analyses_raw_DET)


# calculate effect size for model
es_exploratory_analyses_raw_DET <- lme.dscore(exploratory_analyses_raw_DET,
                                              data = analysis_df, type = "lme4")
```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_raw_DET)

# print effect sizes
es_exploratory_analyses_raw_DET

# print confidence intervals
CI_exploratory_analyses_raw_DET

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_raw_DET)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: does linguistic style change based on the conversation setting?
exploratory_analyses_st_DET <- lmer(DET ~ conv.type + (1|speaker_code),
                                    data = standardized_df, REML = FALSE)

# calculate 95%CI for model
CI_exploratory_analyses_st_DET <- confint(exploratory_analyses_st_DET)

# calculate effect size for model
es_exploratory_analyses_st_DET <- lme.dscore(exploratory_analyses_st_DET,
                                             data = standardized_df, type = "lme4")

```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_st_DET)

# print effect sizes
es_exploratory_analyses_st_DET

# print confidence intervals
CI_exploratory_analyses_st_DET

```


```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_st_DET)

```

We see a difference in DET between conversations
and monologues. Specifically, we find that in conversations *more* function words
occur in sequences than in monologues.

***

### rNRLINE: Exploratory analyses

#### Raw model

```{r exploratory-nrline-model-raw, results="hide", message = FALSE}

# raw: does linguistic style change based on the conversation setting?
exploratory_analyses_raw_rNRLINE <- lmer(rNRLINE ~ conv.type + (1|speaker_code),
                                         data = analysis_df, REML = FALSE)

# calculate 95% CI for model
CI_exploratory_analyses_raw_rNRLINE <- confint(exploratory_analyses_raw_rNRLINE)


# calculate effect size for model
es_exploratory_analyses_raw_rNRLINE <- lme.dscore(exploratory_analyses_raw_rNRLINE,
                                                  data = analysis_df, type = "lme4")
```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_raw_rNRLINE)

# print effect sizes
es_exploratory_analyses_raw_rNRLINE

# print confidence intervals
CI_exploratory_analyses_raw_rNRLINE

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_raw_rNRLINE)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: does linguistic style change based on the conversation setting?
exploratory_analyses_st_rNRLINE <- lmer(rNRLINE ~ conv.type + (1|speaker_code),
                                        data = standardized_df, REML = FALSE)

# calculate 95%CI for model
CI_exploratory_analyses_st_rNRLINE <- confint(exploratory_analyses_st_rNRLINE)

# calculate effect size for model
es_exploratory_analyses_st_rNRLINE <- lme.dscore(exploratory_analyses_st_rNRLINE,
                                                 data = standardized_df, type = "lme4")

```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_st_rNRLINE)

# print effect sizes
es_exploratory_analyses_st_rNRLINE

# print confidence intervals
CI_exploratory_analyses_st_rNRLINE

```


```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_st_rNRLINE)

```

We see a difference in NRLINE between conversations
and monologues. Specifically, we find that there are *more* sequences of
function word use in conversations than in monologues.

***

### MaxL: Exploratory analyses

#### Raw model

```{r exploratory-maxl-model-raw, results="hide", message = FALSE}

# raw: does linguistic style change based on the conversation setting?
exploratory_analyses_raw_maxL <- lmer(maxL ~ conv.type + (1|speaker_code),
                                      data = analysis_df, REML = FALSE)

# calculate 95% CI for model
CI_exploratory_analyses_raw_maxL <- confint(exploratory_analyses_raw_maxL)


# calculate effect size for model
es_exploratory_analyses_raw_maxL <- lme.dscore(exploratory_analyses_raw_maxL,
                                               data = analysis_df, type = "lme4")
```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_raw_maxL)

# print effect sizes
es_exploratory_analyses_raw_maxL

# print confidence intervals
CI_exploratory_analyses_raw_maxL

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_raw_maxL)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: does linguistic style change based on the conversation setting?
exploratory_analyses_st_maxL <- lmer(maxL ~ conv.type + (1|speaker_code),
                                     data = standardized_df, REML = FALSE)

# calculate 95%CI for model
CI_exploratory_analyses_st_maxL <- confint(exploratory_analyses_st_maxL)

# calculate effect size for model
es_exploratory_analyses_st_maxL <- lme.dscore(exploratory_analyses_st_maxL,
                                              data = standardized_df, type = "lme4")

```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_st_maxL)

# print effect sizes
es_exploratory_analyses_st_maxL

# print confidence intervals
CI_exploratory_analyses_st_maxL

```


```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_st_maxL)

```

We see a difference in maxL between conversations
and monologues. Specifically, we find that the longest sequence of
function word use is *longer* in conversations than in monologues.

***

### L: Exploratory analyses

#### Raw model

```{r exploratory-l-model-raw, results="hide", message = FALSE}

# raw: does linguistic style change based on the conversation setting?
exploratory_analyses_raw_L <- lmer(L ~ conv.type + (1|speaker_code),
                                   data = analysis_df, REML = FALSE)

# calculate 95% CI for model
CI_exploratory_analyses_raw_L <- confint(exploratory_analyses_raw_L)


# calculate effect size for model
es_exploratory_analyses_raw_L <- lme.dscore(exploratory_analyses_raw_L,
                                            data = analysis_df, type = "lme4")
```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_raw_L)

# print effect sizes
es_exploratory_analyses_raw_L

# print confidence intervals
CI_exploratory_analyses_raw_L

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_raw_L)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: does linguistic style change based on the conversation setting?
exploratory_analyses_st_L <- lmer(L ~ conv.type + (1|speaker_code),
                                  data = standardized_df, REML = FALSE)

# calculate 95%CI for model
CI_exploratory_analyses_st_L <- confint(exploratory_analyses_st_L)

# calculate effect size for model
es_exploratory_analyses_st_L <- lme.dscore(exploratory_analyses_st_L,
                                           data = standardized_df, type = "lme4")

```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_st_L)

# print effect sizes
es_exploratory_analyses_st_L

# print confidence intervals
CI_exploratory_analyses_st_L

```


```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_st_L)

```

We see a difference in L between conversations
and monologues. Specifically, we find that the average sequence length
is *longer* in conversations than in monologues.

***

### rENTR: Exploratory analyses

#### Raw model

```{r exploratory-rentr-model-raw, results="hide", message = FALSE}

# raw: does linguistic style change based on the conversation setting?
exploratory_analyses_raw_rENTR <- lmer(rENTR ~ conv.type + (1|speaker_code),
                                       data = analysis_df, REML = FALSE)

# calculate 95% CI for model
CI_exploratory_analyses_raw_rENTR <- confint(exploratory_analyses_raw_rENTR)


# calculate effect size for model
es_exploratory_analyses_raw_rENTR <- lme.dscore(exploratory_analyses_raw_rENTR,
                                                data = analysis_df, type = "lme4")
```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_raw_rENTR)

# print effect sizes
es_exploratory_analyses_raw_rENTR

# print confidence intervals
CI_exploratory_analyses_raw_rENTR

```

```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_raw_rENTR)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: does linguistic style change based on the conversation setting?
exploratory_analyses_st_rENTR <- lmer(rENTR ~ conv.type + (1|speaker_code),
                                      data = standardized_df, REML = FALSE)

# calculate 95%CI for model
CI_exploratory_analyses_st_rENTR <- confint(exploratory_analyses_st_rENTR)

# calculate effect size for model
es_exploratory_analyses_st_rENTR <- lme.dscore(exploratory_analyses_st_rENTR,
                                               data = standardized_df, type = "lme4")

```

```{r, eval=TRUE, echo=FALSE}

# print summary tables
summary(exploratory_analyses_st_rENTR)

# print effect sizes
es_exploratory_analyses_st_rENTR

# print confidence intervals
CI_exploratory_analyses_st_rENTR

```


```{r, eval=TRUE, echo=FALSE}

# neatly print output
pander_lme(exploratory_analyses_st_rENTR)

```

We see a difference in rENTR between conversations
and monologues. Specifically, we find that the diversity of sequences is *lower*
in conversations than in monologues.

***

## Post-hoc analyses

Next, we perform several post-hoc analyses to explore whether the changes
observed in the planned model depend on the conversation type (i.e., personal
topics versus conflict).

Again, we do so by creating a series of linear models to analyze whether
conversation type---whether a personal conversation topic (P) or conflict
conversation topic (K)---is connected to the amount of change in a person's
language style from monologue to conversation.

Because each individual contributes only one datapoint to the planned dataset,
we cannot include participant identifier as a random effect in the model. As
a result, we do not include any random effects in the model.

***

### Data preparation

First, we'll need to prepare the data by converting it from long- to wide-form.

```{r post-hoc-prep}

# preparing data for Post-hoc analyses - Bring data into wide format
rqa_mon_post = rqa_mon %>%
  dplyr::rename(conv.type_m = conv.type,
                RR_m = RR,
                DET_m = DET,
                NRLINE_m = NRLINE,
                maxL_m = maxL,
                L_m = L,
                ENTR_m = ENTR,
                rENTR_m = rENTR,
                LAM_m = LAM,
                TT_m = TT,
                rNRLINE_m = rNRLINE)
rqa_conv_post = rqa_conv  %>%
  dplyr::rename(conv.type_c = conv.type,
                cond_c = cond,
                RR_c = RR,
                DET_c = DET,
                NRLINE_c = NRLINE,
                maxL_c = maxL,
                L_c = L,
                ENTR_c = ENTR,
                rENTR_c = rENTR,
                LAM_c = LAM,
                TT_c = TT,
                rNRLINE_c = rNRLINE)

# Calculate difference scores
post_hoc_df = full_join(rqa_mon_post, rqa_conv_post,
                        by=c("dyad_id", "speaker_code")) %>%
  mutate(Diff_RR = RR_m - RR_c,           # positive means higher RR in mon
         Diff_DET = DET_m - DET_c,        # positive means more DET in mon
         Diff_rNRLINE = rNRLINE_m - rNRLINE_c, # positive means more lines in monologue
         Diff_maxL = maxL_m - maxL_c, # positive means a longer  maximal line in monologue
         Diff_L = L_m - L_c, # positive means on average longer lines in monologue
         Diff_rENTR = rENTR_m - rENTR_c,  # positive means more line diversity in monologue
         Diff_LAM = LAM_m - LAM_c, # positive means proportion of points on vertical line is higher in monologue
         Diff_TT = TT_m - TT_c) %>% #positive means on average longer vertical lines in monologue
  
  
  # drop uninformative variables
  select(-conv.type_c, -conv.type_m, -cond) %>%
  
  # update coding for condition
  mutate(cond_c = as.factor(dplyr::if_else(cond_c == "P",
                                           -.5,
                                           .5)))

# save dataframe to file
write.table(post_hoc_df, './data/post_hoc_df.csv',
            sep=",", row.names=FALSE, col.names=TRUE)

# clean up what we don't need
rm(rqa_mon_post, rqa_conv_post)

```

We'll then go ahead and create the raw and standardized dataframes.

```{r create-standardized-post-hoc-dataframe}

# standardize the analysis dataframe
post_hoc_standardized_df = post_hoc_df %>%
  
  # convert things as needed to numeric
  mutate(dyad_id = as.numeric(as.factor(dyad_id)),
         speaker_code = as.numeric(as.factor(speaker_code))) %>%
  
  # standardize
  mutate_all(funs(as.numeric(scale(as.numeric(.))))) %>%
  
  # convert to factors as needed
  mutate(dyad_id = as.factor(dyad_id),
         speaker_code = as.factor(speaker_code),
         cond_c = as.factor(cond_c))

# save dataframe to file
write.table(post_hoc_standardized_df, './data/post_hoc_standardized_df.csv',
            sep=",", row.names=FALSE, col.names=TRUE)

```

***

### RR: Post-hoc analysis

#### Raw model

```{r post-hoc-rr-raw-model, results="hide", message = FALSE}

# raw: do changes in linguistic style between monologues and dialogues
#       differ by conversation type?
post_hoc_RR_raw = lm(Diff_RR ~ cond_c,
                     data = post_hoc_df)

# calculate 95% CI
CI_posthoc_RR_raw <- confint(post_hoc_RR_raw)

# calculate effect size for model
es_post_hoc_RR_raw <- cohensD(x = Diff_RR~cond_c,
                              data = post_hoc_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_RR_raw)

# print confidence intervals
CI_posthoc_RR_raw

# print effect sizes
es_post_hoc_RR_raw

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_RR_raw)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: do changes in linguistic style between monologues and dialogues
#       differ by conversation type?
post_hoc_RR_st = lm(Diff_RR ~ cond_c,
                    data = post_hoc_standardized_df)

# calculate 95% CI
CI_posthoc_RR_st <- confint(post_hoc_RR_st)

# calculate effect size for model
es_post_hoc_RR_st <- cohensD(x = Diff_RR~cond_c,
                             data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_RR_st)

# print confidence intervals
CI_posthoc_RR_st

# print effect sizes
es_post_hoc_RR_st

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_RR_st)

```

We do see that there are significant effects in the change in the overall
amount of recurrence (RR) by conversation type: The change in language style
from monologues to conversation is significantly *lower* when having a friendly
conversation about personal topics (compared to having a conflict conversation
about political topics). In other words, people are *less likely* to change
their language style when having a friendly conversation compared to an
argument.

```{r plot-post-rr-data, echo=FALSE}

qplot(data=post_hoc_df, x=RR_m, y=RR_c, colour=cond_c) +
  geom_abline()

```

***

### DET: Post-hoc analysis

#### Raw model

```{r post-hoc-det-raw-model, results="hide", message = FALSE}

# raw: do changes in structure of linguistic style between monologues and dialogues
#      differ by conversation type?
post_hoc_DET_raw = lm(Diff_DET ~ cond_c,
                      data = post_hoc_df)

# calculate 95% CI
CI_posthoc_DET_raw <- confint(post_hoc_DET_raw)

# calculate effect size
es_post_hoc_DET_raw <- cohensD(x = Diff_DET~cond_c,
                               data = post_hoc_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_DET_raw)

# print confidence intervals
CI_posthoc_DET_raw

# print effect sizes
es_post_hoc_DET_raw

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_DET_raw)

```

#### Standardized model

```{r post-hoc-det-st-model, results="hide", message = FALSE}

# standardized: do changes in structure of linguistic style between monologues and dialogues
#      differ by conversation type?
post_hoc_DET_st = lm(Diff_DET ~ cond_c,
                     data = post_hoc_standardized_df)

# calculate 95% CI
CI_posthoc_DET_st <- confint(post_hoc_DET_st)

# calculate effect size
es_post_hoc_DET_st <- cohensD(x = Diff_DET~cond_c,
                              data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_DET_st)

# print confidence intervals
CI_posthoc_DET_st

# print effect sizes
es_post_hoc_DET_st

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_DET_st)

```

We also see that there are significant effects in the change in the determinism
(DET) by conversation---in other words, the structure in the patterns of
recurrence. Specifically, we see that the change in language style structure
from monologues to conversation is significantly *lower* when followed by a
friendly conversation (compared to a conflict conversation). In other words,
people are *less likely* to change the structure in their language style when
having a friendly conversation as compared to an argumentative one---consistent
with the results found in the post-hoc analysis of RR.

```{r plot-post-det-data, echo=FALSE}

qplot(data=post_hoc_df, x=DET_m, y=DET_c, colour=cond_c) +
  geom_abline()

```

***

### rNRLINE: Post-hoc analysis

#### Raw model

```{r, results="hide", message = FALSE}

# raw: do changes in amount of structure of linguistic style between
#      monologues and dialogues differ by conversation type?
post_hoc_rNRLINE_raw = lm(Diff_rNRLINE ~ cond_c,
                          data = post_hoc_df)

# calculate 95% CI
CI_posthoc_rNRLINE_raw <- confint(post_hoc_rNRLINE_raw)

# calculate effect size
es_post_hoc_rNRLINE_raw <- cohensD(x = Diff_rNRLINE~cond_c,
                                   data = post_hoc_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_rNRLINE_raw)

# print confidence intervals
CI_posthoc_rNRLINE_raw

# print effect sizes
es_post_hoc_rNRLINE_raw

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_rNRLINE_raw)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: do changes in amount of structure of linguistic style between
#      monologues and dialogues differ by conversation type?
post_hoc_rNRLINE_st = lm(Diff_rNRLINE ~ cond_c,
                         data = post_hoc_standardized_df)

# calculate 95% CI
CI_posthoc_rNRLINE_st <- confint(post_hoc_rNRLINE_st)

# calculate effect size
es_post_hoc_rNRLINE_st <- cohensD(x = Diff_rNRLINE~cond_c,
                                  data = post_hoc_standardized_df)


```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_rNRLINE_raw)

# print confidence intervals
CI_posthoc_rNRLINE_st

# print effect sizes
es_post_hoc_rNRLINE_st

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_rNRLINE_st)

```

```{r, echo=FALSE}

qplot(data=post_hoc_df, x=rNRLINE_m, y=rNRLINE_c, colour=cond_c) +
  geom_abline()

```

In contrast with RR and DET, we do not see a
difference in the normalized number of lines on the RP by conversation type. Essentially, this is another measure of continued structure within a
system by capturing the total number of lines in the plot.

***
### MaxL: Post-hoc analysis

#### Raw model

```{r, results="hide", message = FALSE}

# raw: do changes in uniformity of structure of linguistic style between monologues
#       and dialogues differ by conversation type?
post_hoc_maxL_raw = lm(Diff_maxL ~ cond_c,
                       data = post_hoc_df)

# calculate 95% CI
CI_posthoc_maxL_raw <- confint(post_hoc_maxL_raw)

# calculate effect size
es_post_hoc_maxL_raw <- cohensD(x = Diff_maxL~cond_c,
                                data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_maxL_raw)

# print confidence intervals
CI_posthoc_maxL_raw

# print effect sizes
es_post_hoc_maxL_raw

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_maxL_raw)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: do changes in uniformity of structure of linguistic style between monologues
#       and dialogues differ by conversation type?
post_hoc_maxL_st = lm(Diff_maxL ~ cond_c,
                      data = post_hoc_standardized_df)

# calculate 95% CI
CI_posthoc_maxL_st <- confint(post_hoc_maxL_st)

# calculate effect size
es_post_hoc_maxL_st <- cohensD(x = Diff_maxL~cond_c,
                               data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_maxL_st)

# print confidence intervals
CI_posthoc_maxL_st

# print effect sizes
es_post_hoc_maxL_st

```


```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_maxL_st)

```

Here, we see that there are significant effects in the change in the longest
diagonal line segment (maxL). Specifically, we see that the change in
sequential patterns of function word use is significantly *lower* when followed
by a friendly conversation. Put together, the change in the longest sequence of
function words is smaller when having a friendly conversation as compared to an
argumentative one.


```{r, echo=FALSE}

qplot(data=post_hoc_df, x=maxL_m, y=maxL_c, colour=cond_c) +
  geom_abline()

```

***

### L: Post-hoc analysis

#### Raw model

```{r, results="hide", message = FALSE}

# raw: do changes in uniformity of structure of linguistic style between monologues
#       and dialogues differ by conversation type?
post_hoc_L_raw = lm(Diff_L ~ cond_c,
                    data = post_hoc_df)

# calculate 95% CI
CI_posthoc_L_raw <- confint(post_hoc_L_raw)

# calculate effect size
es_post_hoc_L_raw <- cohensD(x = Diff_L~cond_c,
                             data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_L_raw)

# print confidence intervals
CI_posthoc_L_raw

# print effect sizes
es_post_hoc_L_raw

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_L_raw)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: do changes in uniformity of structure of linguistic style between monologues
#       and dialogues differ by conversation type?
post_hoc_L_st = lm(Diff_L ~ cond_c,
                   data = post_hoc_standardized_df)

# calculate 95% CI
CI_posthoc_L_st <- confint(post_hoc_L_st)

# calculate effect size
es_post_hoc_L_st <- cohensD(x = Diff_L~cond_c,
                            data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_L_st)

# print confidence intervals
CI_posthoc_L_st

# print effect sizes
es_post_hoc_L_st

```


```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_L_st)

```

We see that there are significant effects in the change in the average line
length (L). Specifically, we see that the change in average line length
(sequences of uninterrupted function word use) is significantly *lower* when
followed by a friendly conversation. In other words, the change in the average
uninterrupted sequence of function word use is smaller when having a friendly
conversation as compared to an argumentative one.

```{r, echo=FALSE}

qplot(data=post_hoc_df, x=L_m, y=L_c, colour=cond_c) +
  geom_abline()

```

***

### rENTR: Post-hoc analysis

#### Raw model

```{r, results="hide", message = FALSE}

# raw: do changes in uniformity of structure of linguistic style between monologues
#       and dialogues differ by conversation type?
post_hoc_rENTR_raw = lm(Diff_rENTR ~ cond_c,
                        data = post_hoc_df)

# calculate 95% CI
CI_posthoc_rENTR_raw <- confint(post_hoc_rENTR_raw)

# calculate effect size
es_post_hoc_rENTR_raw <- cohensD(x = Diff_rENTR~cond_c,
                                 data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_rENTR_raw)

# print confidence intervals
CI_posthoc_rENTR_raw

# print effect sizes
es_post_hoc_rENTR_raw

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_rENTR_raw)

```

#### Standardized model

```{r, results="hide", message = FALSE}

# standardized: do changes in uniformity of structure of linguistic style between monologues
#       and dialogues differ by conversation type?
post_hoc_rENTR_st = lm(Diff_rENTR ~ cond_c,
                       data = post_hoc_standardized_df)

# calculate 95% CI
CI_posthoc_rENTR_st <- confint(post_hoc_rENTR_st)

# calculate effect size
es_post_hoc_rENTR_st <- cohensD(x = Diff_rENTR~cond_c,
                                data = post_hoc_standardized_df)

```

```{r, eval=TRUE, echo=FALSE}

# print summary table
summary(post_hoc_rENTR_st)

# print confidence intervals
CI_posthoc_rENTR_st

# print effect sizes
es_post_hoc_rENTR_st

```

```{r, echo=FALSE}

# neatly print output
pander_lm(post_hoc_rENTR_st)

```

Unlike RR and DET, we do not see a difference in normalized entropy by
conversation type. Normalized entropy essentially captures the degree to which
the structure of the line lengths are uniform (i.e., lower variety in line
lengths means lower rENTR) or more heterogenous (i.e., higher variety in line
lengths means higher rENTR).

```{r, echo=FALSE}

qplot(data=post_hoc_df, x=rENTR_m, y=rENTR_c, colour=cond_c) +
  geom_abline()

```

***
